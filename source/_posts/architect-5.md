---
title:  JavaWeb笔记-LVS负载均衡 D1
date: 2020-07-19 22:26:19
categories:
- [Java开发]
tags:
- [LVS]
---

我们暂时跳过FastDSF集群的搭建，先来看一下其他的负载均衡技术，这节笔记包括以下内容：LVS简介、LVS的三种负载均衡的模型

<!--more-->

似乎上学期间翘课太多了，并且毕业以后知识都还给老师了，计算机网络这门课现在能记住的东西真的不多😂，之前我们提到过负载均衡可以提高服务器处理并发的能力，也就是如果一台服务器面对大量的请求忙不过来，我们就多加几台服务器一起顶上去，并通过一个应用来分流转发请求给这些服务器，之前是通过Nginx来实现的，这里学习另外一种负载均衡的实现方式也就是标题的LVS

## LVS简介
LVS（Linux Virtual Server）是指Linux虚拟服务器。这是一个由章文嵩博士发起的一个开源项目，[这个是它的官网](http://www.linuxvirtualserver.org)， 目前 LVS 已经是 Linux 内核标准的一部分。使用 LVS 可以达到的技术目标是：通过 LVS 和 Linux 操作系统实现一个高性能高可用的 Linux 服务器集群，它具有良好的可靠性、可扩展性和可操作性，从而以低廉的成本实现最优的性能。

**Nginx与LVS负载均衡的区别**

首先最本质的区别是它们在网络模型中所处的层级是不同的，可以说其他的区别都是从这点展开说的，Nginx在第七层应用层之上做负载均衡，而LVS是在第四层做负载均衡，通常来说越接触底层效率越高，所以LVS的性能更为高效，可以支持十几万的并发，而Nginx在实际的生产环境中大约能扛下1-5万的并发量。因为Nginx是在第7层，也就是可以看到的更多的请求细节，所以可以对Http应用本身来做分流策略，比如针对域名、目录结构等，相比之下LVS并不具备这样的功能；Nginx安装和配置非常简单，测试起来也很方便，因为它基本能把错误用日志打印出来，而LVS的安装和配置、测试就要花比较长的时间了，因为LVS对网络依赖比较大，很多时候不能配置成功都是因为网络问题而不是配置问题，出了问题要解决也相应的会麻烦得多。

查询了一些博客提到对于Nginx与LVS的选型，一般是前期先是通过Nginx做单点的负载均衡，当业务规模庞大后使用LVS，将Nginx作为LVS负载的节点，最后再进一步发展后来定制专门的负载均衡方案，或者购买商业负载均衡，希望未来自己搞自己的产品也能做到这一步哈哈。

## LVS负载均衡的模型

### 相关术语

- `DS` : Director Server 指负载均衡服务器
- `RS` : Real Server 指真实的业务服务器
- `CIP` : Client IP 指客户端IP
- `VIP` : Virtual IP 指用户请求的目标IP地址
- `DIP` : Director IP 指用于集群内通信的主机IP
- `RIP` : Real IP 指后端服务器的真实IP地址

### LVS三种负载均衡模型

这几简单介绍三种LVS负载模型流程来认识它的工作原理，穿插需要的机网知识，~~原计划是系统整理一下但是发现东西太多，几乎是贯穿了整个计算机网络这门课~~，三种常用的常用的模型分别是NAT、RD、TUN

#### 小小的前置

**网络层-路由表**

我们知道将多台计算机连接在一起组成了网络，而将多个网络通过路由器连接在一起就构成了互联网，那么互联网中计算机A是怎样把数据包发送给计算机B？我们用下图表示一个简单的通信模型

<img src="/img/java/LVSROUTER.png"  width="90%" style="text-align:center">

首先要知道计算机B的IP地址，但是数据包如何从A一步一步确定路由路径发送给计算机B，我们肯定无法事先保存A到B的完整路径，所以计算机是通过获取数据包下一跳的来确定的，就是数据包每到一个节点再确定路由的下一个点是什么，最终一步步发送到计算机B上，每个节点中都有一个路由表来保存跳转关系，通过这个表就知道数据包该发向谁。在Linux系统中使用`route -n` 命令可以查询路由表

假设此时计算机A的路由表如下：

| Destination | Gateway | Genmask | Flags | Metric | Ref | Use | Iface |
| :----: |:----: |:----: |:----: |:----: |:----:|:----:|:----:|
|0.0.0.0  |172.16.31.253|0.0.0.0 |UG|100|0|0|eth0|
|172.16.16.0|0.0.0.0|255.255.240.0|U|100|0|0|eth0|

计算的下一跳方式是通过目标IP与路由表中每一条记录的子网掩码(Genmask)进行按位与来得到目标的网络号，如果得到的目标网络号与该记录的Destination相同表示该数据包应该发给这条记录的网关(Gateway)

由于此时路由表中第一条记录的genmask是0.0.0.0，不论什么ip与这个地址进行与运算得到的都是0.0.0.0，数据包都会通过这个网关发出去，0.0.0.0我们称为默认网关，一般这个网关连接的是路由器，也就是说你的数据包第一跳发给你的路由器，接着它再查询路由器的路由表继续跳，直到跳到某一个路由器计算机B连接这个路由器，那么数据包就可以在这个路由器的路由表中找到B并发送给它。

这里因为篇幅 ~~其实是我讲不清楚，查明白再写~~，不介绍路由表是如何产生维护的


**链路层-MAC地址**

上一节我们提到数据包在网络层通过下一跳的方式寻找路由，而实际传递转发数据包时需到数据链路层，这一层为了正常工作为数据包加了新的字段mac地址，mac地址是一个硬件地址，可以简单理解为一个路由器有一个确定的mac地址；我们在确定了数据包的下一跳地址，在数据包中会添加源mac地址与目标mac地址，源mac地址是当前这个路由器的mac地址，目标mac地址是下一跳路由器mac地址，这样数据包就会传输到下一个路由器中，每到下一跳源mac与目标mac地址就会改变；整个数据包在互联网中进行一跳一跳传输时，数据包中的源IP/目标IP地址字段是不会改变的，而数据包中源mac/目标mac在每一跳都会改变

#### NAT

NAT的架构图如下：

<img src="/img/java/LVS1.png"  width="90%" style="text-align:center">

NAT模型是指从客户端网关发出的数据包全部发送给DS负载均衡服务器，DS将数据包转发给RS服务器，因为LVS是在四层，导致DS不会与client发生三次握手建立连接，DS的目的只有转发这个数据包到RS中，所以处理起来特别快，同样是因为四层缘故，所以它能解析的数据只有数据包中IP信息与MAC地址信息，其余信息比如端口号url是获取不到的，例如所此时它获取的数据包中部分信息如下
```
源IP:CIP
目标IP:VIP
```
如果DS此时直接把数据包转发给RS，RS是不会处理的，因为数据包中`目标IP`是DS的IP，RS拿到这个数据包检查IP后发现不是发给自己的就会直接丢弃，所以DS中应该需要做一步网络地址转换（Network Address Translation,简写NAT）也就是从DS中转发的数据包IP部分信息改写为如下内容：
```
源IP:CIP
目标IP:RIP
```

最后由RS服务器拿到数据包，建立连接处理请求，此时服务器建立的是从`CIP -> RIP`的一个连接，处理完成后返回结果信息，其中返回数据包中IP部分的信息是
```
源IP:RIP
目标IP:CIP
```
那么问题来了，如果直接将这个数据包返回给客户端，客户端是不会处理的，因为从客户端的角度看：我把数据包发给了VIP，但是结果从RIP这个地址给我返回了一个数据包，根据通信协议的规定这个数据包会被丢弃，为了能使模型正常工作这个数据包的源IP信息应该改回VIP，这又是个NAT地址转换任务同样也是DS服务器做的，因为它的内部记录了转换的规则，现在的问题就是如何做到从RS中返回的数据包要传递给DS，根据我们小学二年级就学过的网络层-路由表知识可得：只要在RS的路由表中将默认网关直接或者间接指向DS服务器就可以了，这样不论什么数据包都会发送给DS服务器，DS获取数据包改源IP地址从RIP为VIP，在返回给客户端就完成了整个的负载均衡的模型

这种方式需要做网络地址转换，所以也叫NAT模型；通过NAT我们了解到作为负载均衡服务器应该能够快速的转发数据包，正是因为LVS是在网络模型第四层它不会继续与client建立握手连接，而是直接转发数据包，所以能够快速的处理数据

这种模型虽然实现了 LVS 负载均衡，但是它也有一些问题，首先由于所有的数据包来回都要通过 DS 服务器，所以 DS 的服务器带宽就成为了瓶颈，通常通信是非对称的，也就是下行的数据量要比上行的数据量大；另外就是因为需要 DS 做网络地址转换，所以对 DS 有一定的算力要求

#### DR模型

DR的架构图如下：

<img src="/img/java/LVS2.png"  width="90%" style="text-align:center">

DR小名直接路由模型，我们通过特殊的手段在RS服务器上配置VIP的IP地址，这个地址只有本服务器知道，其他服务器不知道，当DS拿到数据包时不再需要转换目标IP地址，直接转发给RS就可以，这个转发过程就是直接修改mac地址，DS拿到数据包只需要解开两层（链路层），然后修改这个数据包的源mac/目标mac地址为DS的mac地址/RS的mac地址，这样这个数据包就跳到RS上了；相比与NAT，NAT需要解开三层（网络层）修改目标IP地址再转发，这样就减少了算力的消耗；不过DR有一个要求就是DS与RS需要在同一个局域网内，这样直接改mac地址才有用，这点很好理解不在同一个局域网内DS是没办法只修改一次mac地址就让数据包跳到RS上了，最后在RS中建立的连接是从CIP到VIP的，因为我们配置了RS的IP就是VIP，所以RS返回的数据包可以不走DS直接返回给client，这样就减轻了 DS 带宽瓶颈的问题，下行带宽被分流到了各个服务器上

通常DR的模型使用是最广泛的

#### TUN

TUN的架构图如下：

<img src="/img/java/LVS3.png"  width="90%" style="text-align:center">

使用DR模型要求DS与所有的RS在同一个局域网内，TUN模型是使用了隧道技术解除了这个限制。DR中若DS与RS不在统一局域网内，正常情况下DS转发出的数据包会根据默认网关跳到DS连接的路由器中，接着在路由器中查询下一跳路由就会返回到DS中，因为数据包记录的目标IP就是VIP，也就是DS网卡的IP地址；这里为了使RS与DS不在同一局域网还能正常转发需要用到隧道技术TUN

简单来说隧道技术就是把真实的源IP/目标IP外面又包一层源IP/目标IP，这样你的数据包就会先发给外面包裹的这个目标IP，服务器解开这个包拿到里面的真实源IP/目标IP，然后再继续正常的传输。这个场景最熟悉的应用就应该是VPN了，如果你想访问Google，那么你发出的数据包里源IP是你自己的IP/目标IP是Google的IP，正常这个访问会被墙，如果你有一台代理服务器，这个代理服务器是可以向Google发送数据包，那么就可以用隧道技术把你访问Google的请求先包起来标记上代理服务器的IP发送给代理服务器，由代理服务器再转发你的请求给Google来实现科学上网

所以回到TUN的模型中，由DS转发的数据包包裹一下，如图中所示，这个数据包就会发送给RS，RS拆包发现里面的数据包就是发送给自己的（自己配置虚拟的VIP），通过这种方式实现了DS与RS不在同一局域网呢也可以正常转发数据包












